#Elephants Don't Play Chess 
###### Rodney A. Brooks

## Keywords:
1. **Situated cognition** is a theory that points out that knowing is inseparable from doing.
2. **Subsumption architecture** is basedon how evolution took place. Not as complicated as humans but serves the purpose. The layers in this are parallel unlike humans which is sequential
3. **inputs are supressed and outputs are inhibited**
4. **LISP Machine** general-purpose computers designed to efficiently run Lisp as their main software and programming language, usually via hardware support. They are an example of a high-level language computer architecture, and in a sense, they were the first commercial single-user workstations.


The ML we are doing now is symbolic or general intelligence. It needs symbols as inputs where it tries to understand the symbol's architecture. That understanding part is the intelligence. This is how our brain works and we are trying to get that into computers.

But the mind is also a decentralized collection of processors where each does it's own activity and if a higher or complex level activity is needed, we makes use of these lower level activities.

Brook's view is opposite. He proves classical AI way is non-robust and difficult and simple architecture is sufficient. By building a layered stack where the above layer subsumes(makes use of) below layer but not vice versa, it is possible to mimic a housefly completely which is better that toady's AI that models housefly's brain.

The code for a full functional robot to pick up cans is written this way:
1. a code that is able to roam freely or follow a wall or a code that operates the arm.
2. The top layer looks something like this : it detects a can and uses the below layers to finish the task. In terms of programming, its like we have an object with many subroutines and when needed, they are invoked.


 


## Definitions:

## Introduction:
The current research in artificial intelligence seems to be saturating in terms of effencient mathematical models. Brooks argues that the reason behind this is the assumptions and methods we started off with that is the "symbolic system".

But there seems to be another way to infer intelligence and implement the same in machines. Unlike the classical approach of modelling various components as function blocks, the new or nouvelle AI models behavioural blocks. This way, the perfomance improves as we keep adding behavioural modules. But in classical AI, the effiency needs to be increrased to improve the system.

## Architecture
it's more like heart an mind fighting. We suddenly get a desire to do something. But then we think if doin that is right or not.

We are assigning numbers and symbols, ie, simulating a mathematical environment to perform operations. But the application lies completely in the real world. When the whole system is ported to real world scenario, it might not perform as expected. But the "physical grounding hypothesis" uses the inputs from real world from the beginning itself. so, at the end of training (used loosely), the system is ready to face the real world.

debugging is easy as we can literally see it with our eyes when it's performing.

As the lower layers are subsumed, every layer must be near to perfect. Else, there'll be drastic change in output. Unlike classical AI.

Funny analogy: Ambani vs Grads of Ivy League B-schools.
## Verdict


